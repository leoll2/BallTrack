/*
 * pilot.c
 *
 *  Created on: 11/feb/2019
 *      Author: leona
 */

#include "pilot.h"

/* These values are obtained from reinforcement_learning.py
 * which exploits Q-Learning algorithm to generate them.
 * Each number, in range [0,7], corresponds to a direction */
static const ActionCode best_actions[N_BOX_Y][N_BOX_X] = {
		{0, 0, 0, 4, 4, 5, 5, 6, 6, 0, 0, 0, 0, 0, 0, 4, 4, 4, 5, 6, 6},
		{0, 0, 0, 4, 4, 5, 5, 6, 6, 0, 0, 0, 0, 0, 0, 4, 4, 3, 0, 7, 6},
		{0, 0, 0, 4, 4, 5, 5, 6, 7, 0, 0, 0, 0, 0, 0, 3, 3, 2, 1, 0, 0},
		{0, 0, 0, 4, 4, 5, 5, 6, 6, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0},
		{0, 0, 0, 4, 4, 5, 5, 6, 7, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0},
		{0, 0, 0, 4, 4, 5, 5, 6, 6, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0},
		{4, 4, 4, 5, 4, 5, 5, 6, 6, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0},
		{4, 4, 4, 5, 5, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0},
		{3, 4, 4, 5, 5, 6, 6, 6, 7, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0},
		{4, 4, 4, 5, 6, 7, 7, 7, 7, 0, 0, 0, 4, 4, 4, 2, 2, 1, 1, 0, 0},
		{4, 4, 4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 2, 1, 1, 0, 0},
		{4, 4, 4, 5, 6, 6, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 2, 1, 1, 0, 0},
		{4, 4, 4, 5, 6, 6, 0, 0, 0, 0, 0, 0, 3, 2, 2, 2, 1, 1, 1, 0, 0},
		{4, 4, 4, 5, 6, 6, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 1, 0, 0},
		{4, 4, 4, 5, 6, 6, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 1, 0, 0, 0},
		{4, 3, 4, 5, 6, 4, 5, 6, 6, 0, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0},
		{4, 3, 3, 4, 4, 4, 5, 6, 6, 0, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0},
		{4, 3, 3, 3, 4, 4, 5, 6, 6, 0, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0},
		{3, 3, 3, 3, 3, 4, 5, 6, 4, 4, 4, 4, 2, 2, 1, 1, 0, 0, 0, 0, 0},
		{2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 2, 1, 1, 0, 0, 0, 0, 0},
		{2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 1, 0, 0, 0, 0, 0},
		{0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 1, 1, 1, 0, 0, 0, 0, 0},
		{0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0},
		{0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 2, 1, 0, 0, 0, 0, 0},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0}
};


struct delta {
	int16_t i;
	int16_t j;
};

static struct delta action_delta[N_ACTION] = {
		{-1, -1},
		{0, -1},
		{1, -1},
		{1, 0},
		{1, 1},
		{0, 1},
		{-1, 1},
		{-1, 0}
};


/* Return the code of the best action from state b. */
ActionCode Pilot_GetBestAction(Box b)
{
	return best_actions[b.j][b.i];
}


Box Pilot_GetNextBox(Box cur, ActionCode a)
{
	struct delta d = action_delta[a];
	return Field_GetBoxFromDelta(cur, d.i, d.j);
}
